{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milan-banura/LeadScoring_CaseStudy_ML1/blob/main/Leads_Scoring_CaseStudy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9bAK7zzGMUO"
      },
      "source": [
        "# Lead Score - Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZjkUmgtA8fK"
      },
      "source": [
        "<b><font color = maroon>Problem Statement</font></b><br>\n",
        "\n",
        "<p align=\"justify\">X Education is an education company that offers online courses for industry professionals. The company attracts many visitors to its website through various marketing channels. The company faces a problem: its lead conversion rate is very low. Out of 100 leads, only 30 become customers on average.</p>\n",
        "\n",
        "<p align=\"justify\">To solve this problem, X Education wants to identify the most potential leads, also known as ‘Hot Leads’. The company has hired you to help them with this task. Your job is to build a model that can assign a lead score to each lead based on various factors, such as their demographics, behavior, preferences, etc. The higher the lead score, the more likely the lead is to convert. The lower the lead score, the less likely the lead is to convert. The company’s CEO has set a target of achieving an 80% lead conversion rate with this model.</p>\n",
        "\n",
        "<b><font color = maroon>Goals and Objective</font></b><br>\n",
        "- <p align=\"justify\">Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.</p>\n",
        "- <p align=\"justify\">There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oINYuwHCNwQ"
      },
      "source": [
        "#### Steps Followed  \n",
        "- Reading Data\n",
        "- Cleaning Data\n",
        "- Data Visualization\n",
        "- Data Preparation\n",
        "- Model Builiding\n",
        "- ROC Curve\n",
        "- Model Evaluations\n",
        "- Prediction on test set\n",
        "- Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_PCJny-H2Xp"
      },
      "source": [
        "## Step 1: Reading and Understanding the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCYLTXoiA1aN"
      },
      "outputs": [],
      "source": [
        "# Supress Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Importing required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiJNdDvyHRVY"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/milan-banura/LeadScoring_CaseStudy_ML1/main/Leads.csv\"\n",
        "lead_df = pd.read_csv(url)\n",
        "# Reading leads dataframe as lead_df\n",
        "\n",
        "lead_df_original = lead_df.copy()\n",
        "lead_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur23fs0BJiEA"
      },
      "outputs": [],
      "source": [
        "lead_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2I7m8VJH0pg"
      },
      "outputs": [],
      "source": [
        "# Inspect the various aspects of the data dataframe\n",
        "lead_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFNgSG28uQN"
      },
      "source": [
        "## Step 2: Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVbs-i0w_xfP"
      },
      "outputs": [],
      "source": [
        "# To check for duplicates\n",
        "lead_df.loc[lead_df.duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1XPJvcG_4cA"
      },
      "source": [
        "#### No duplicates in the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEt0_H1gAHMo"
      },
      "outputs": [],
      "source": [
        "# To check for duplicates in columns\n",
        "print(sum(lead_df.duplicated(subset = 'Lead Number')))\n",
        "print(sum(lead_df.duplicated(subset = 'Prospect ID')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHue1vL7AP6X"
      },
      "source": [
        "#### As the values in these columns are different for each entry/row, there are just indicative of the ID and are not important from an analysis point of view. Hence, can be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDxVv31zAXzL"
      },
      "outputs": [],
      "source": [
        "lead_df = lead_df.drop(['Lead Number','Prospect ID'],axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6qo-tO9RyW"
      },
      "source": [
        "#### 'Select' seems to be the default value stored in the backend for columns that are optional in nature and the prospective lead has chosen not to select any of options available in the dropdown menu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMZfEsEc-GP3"
      },
      "outputs": [],
      "source": [
        "# To convert 'Select' values to NaN\n",
        "lead_df = lead_df.replace('Select', np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4haja6IAA1Jz"
      },
      "outputs": [],
      "source": [
        "# To get percentage of null values in each column\n",
        "round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbJEJYNBCHR"
      },
      "source": [
        "#### We'll drop columns with more than 50% of missing values as it does not make sense to impute these many values. But the variable 'Lead Quality', which has 51.6% missing values seems promising. So we'll keep it for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzFgIqvFBiSm"
      },
      "outputs": [],
      "source": [
        "# To drop columns with more than 50% of missing values as it does not make sense to impute these many values\n",
        "lead_df = lead_df.drop(lead_df.loc[:,list(round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)>52)].columns, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBw0nlGfH04c"
      },
      "outputs": [],
      "source": [
        "round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWd4eeNSJ8wI"
      },
      "source": [
        "\n",
        "#### For other columns, we have to work on column by column basis.  \n",
        "- For categorical variables, we'll analyse the count/percentage plots.\n",
        "- For numerical variable, we'll describe the variable and analyse the box plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEPs0_rIORE0"
      },
      "outputs": [],
      "source": [
        "# Function for percentage plots\n",
        "def percent_plot(var):\n",
        "    values = (lead_df[var].value_counts(normalize=True)*100)\n",
        "    plt_p = values.plot.bar(color=sns.color_palette('deep'))\n",
        "    plt_p.set(xlabel = var, ylabel = '% in dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6_SkUCjOUfd"
      },
      "outputs": [],
      "source": [
        "# For Lead Quality\n",
        "percent_plot('Lead Quality')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ehzVR9Oj2c"
      },
      "source": [
        "### Null values in the 'Lead Quality' column can be imputed with the value 'Not Sure' as we can assume that not filling in a column means the employee does not know or is not sure about the option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9CCs7ZvOV4F"
      },
      "outputs": [],
      "source": [
        "lead_df['Lead Quality'] = lead_df['Lead Quality'].replace(np.nan, 'Not Sure')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5Z_2yMO2rH"
      },
      "outputs": [],
      "source": [
        "# For 'Asymmetrique Activity Index', 'Asymmetrique Profile Index', 'Asymmetrique Activity Score', 'Asymmetrique Profile Score'\n",
        "asym_list = ['Asymmetrique Activity Index', 'Asymmetrique Profile Index', 'Asymmetrique Activity Score', 'Asymmetrique Profile Score']\n",
        "plt.figure(figsize=(10, 7))\n",
        "for var in asym_list:\n",
        "    plt.subplot(2,2,asym_list.index(var)+1)\n",
        "    if 'Index' in var:\n",
        "        sns.countplot(data=lead_df, x=var)\n",
        "    else:\n",
        "        sns.boxplot(data=lead_df, x=var)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSkvuiACVBfh"
      },
      "outputs": [],
      "source": [
        "# To describe numerical variables\n",
        "lead_df[asym_list].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs71XBwFVPyP"
      },
      "source": [
        "#### These four variables have more than 45% missing values and it can be seen from the plots that there is a lot of variation in them. So, it's not a good idea to impute 45% of the data. Even if we impute with mean/median for numerical variables, these values will not have any significant importance in the model. We'll have to drop these variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ2CuQEUVQLl"
      },
      "outputs": [],
      "source": [
        "lead_df = lead_df.drop(asym_list,axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otTLYEK9VuKc"
      },
      "outputs": [],
      "source": [
        "# To see percentage of null values in each column\n",
        "round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdkdC3hfVyJx"
      },
      "outputs": [],
      "source": [
        "# For 'City'\n",
        "percent_plot('City')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m047HvHOV6Hr"
      },
      "source": [
        "#### Around 60% of the City values are Mumbai. We can impute 'Mumbai' in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bllWqExfV154"
      },
      "outputs": [],
      "source": [
        "lead_df['City'] = lead_df['City'].replace(np.nan, 'Mumbai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYNxdPuWCGF"
      },
      "outputs": [],
      "source": [
        "# For 'Specialization'\n",
        "percent_plot('Specialization')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o73XtOluWLGa"
      },
      "source": [
        "#### There are a lot of different specializations and it's not accurate to directly impute with the mean. It is possible that the person does not have a specialization or his/her specialization is not in the options. We can create a new column for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85_n9FYtWErD"
      },
      "outputs": [],
      "source": [
        "lead_df['Specialization'] = lead_df['Specialization'].replace(np.nan, 'Others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqla7ad2Wb05"
      },
      "outputs": [],
      "source": [
        "# For 'Tags', 'What matters most to you in choosing a course', 'What is your current occupation' and 'Country'\n",
        "var_list = ['Tags', 'What matters most to you in choosing a course', 'What is your current occupation', 'Country']\n",
        "plt.figure(figsize=(15, 7))\n",
        "for var in var_list:\n",
        "    plt.subplot(2,2,var_list.index(var)+1)\n",
        "    percent_plot(var)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqu2ojbNid8L"
      },
      "source": [
        "#### In all these categorical variables, one value is clearly more frequent than all others. So it makes sense to impute with the most frequent values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOcHub5QWjdt"
      },
      "outputs": [],
      "source": [
        "# To impute with the most frequent value\n",
        "for var in var_list:\n",
        "    top_frequent = lead_df[var].describe()['top']\n",
        "    lead_df[var] = lead_df[var].replace(np.nan, top_frequent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzo5rPJnjGQ1"
      },
      "outputs": [],
      "source": [
        "# Let's see percentage of null values in each column\n",
        "round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L49A2EIkj8F4"
      },
      "outputs": [],
      "source": [
        "# For 'TotalVisits' and 'Page Views Per Visit'\n",
        "visit_list = ['TotalVisits', 'Page Views Per Visit']\n",
        "plt.figure(figsize=(15, 5))\n",
        "for var in visit_list:\n",
        "    plt.subplot(1,2,visit_list.index(var)+1)\n",
        "    sns.boxplot(data=lead_df, x=var)\n",
        "plt.show()\n",
        "\n",
        "lead_df[visit_list].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkNra_CkePo"
      },
      "source": [
        "#### From the above analysis, it can be seen that there is a lot of variation in both of the variables. As the percentage of missing values for both of them are less than 2%, it is better to drop the rows containing missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANX4ej4Ikh7j"
      },
      "outputs": [],
      "source": [
        "# For 'Lead Source' and 'Last Activity'\n",
        "var_list = ['Lead Source', 'Last Activity']\n",
        "\n",
        "for var in var_list:\n",
        "    percent_plot(var)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoO421GMk-8c"
      },
      "source": [
        "#### In these categorical variables, imputing with the most frequent value is not accurate as the next most frequent value has similar frequency. Also, as these variables have very little missing values, it is better to drop the rows containing these missing values. Hence, we'll drop the rows containing any missing missing values for above four variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7T8JvZKk6jH"
      },
      "outputs": [],
      "source": [
        "# To drop the rows containing missing values\n",
        "lead_df.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uYpsI8tlECg"
      },
      "outputs": [],
      "source": [
        "# Let's see percentage of null values in each column\n",
        "round(100*(lead_df.isnull().sum()/len(lead_df.index)), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUH-F5bdlRPw"
      },
      "source": [
        "#### Great! No more missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S85Zg-X-mFtF"
      },
      "source": [
        "## Step 3: Data Visualtization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd-ww94GmPW2"
      },
      "outputs": [],
      "source": [
        "# For the target variable 'Converted'\n",
        "percent_plot('Converted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Q-sz7AnNnw"
      },
      "outputs": [],
      "source": [
        "(sum(lead_df['Converted'])/len(lead_df['Converted'].index))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CStoAh62nejj"
      },
      "source": [
        "#### 37.8% of the 'Converted' data is 1 ie. 37.8% of the leads are converted. This means we have enough data of converted leads for modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDPG_rkpooDL"
      },
      "source": [
        "### Visualising Numerical Variables and Outlier Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1ieDG-unsAA"
      },
      "outputs": [],
      "source": [
        "# Boxplots\n",
        "num_var = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n",
        "plt.figure(figsize=(15, 10))\n",
        "for var in num_var:\n",
        "    plt.subplot(3,1,num_var.index(var)+1)\n",
        "    sns.boxplot(data=lead_df, x=var)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Ek-e8XodgI"
      },
      "outputs": [],
      "source": [
        "lead_df[num_var].describe([0.05,.25, .5, .75, .90, .95])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr2FKhzmo5SW"
      },
      "source": [
        "#### From the boxplots, we can see that there are outliers present in the variables.\n",
        "- For 'TotalVisits', the 95% quantile is 10 whereas the maximum value is 251. Hence, we should cap these outliers at 95% value.\n",
        "- There are no significant outliers in 'Total Time Spent on Website'\n",
        "- For 'Page Views Per Visit', similar to 'TotalVisits', we should cap outliers at 95% value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bB6aSUHpIUu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi9sspfFpfdD"
      },
      "outputs": [],
      "source": [
        "# Outlier treatment\n",
        "percentile = lead_df['TotalVisits'].quantile([0.95]).values\n",
        "lead_df['TotalVisits'][lead_df['TotalVisits'] >= percentile[0]] = percentile[0]\n",
        "\n",
        "percentile = lead_df['Page Views Per Visit'].quantile([0.95]).values\n",
        "lead_df['Page Views Per Visit'][lead_df['Page Views Per Visit'] >= percentile[0]] = percentile[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFUgqQ7m2zfj"
      },
      "outputs": [],
      "source": [
        "# Plot Boxplots to verify\n",
        "plt.figure(figsize=(15, 10))\n",
        "for var in num_var:\n",
        "    plt.subplot(3,1,num_var.index(var)+1)\n",
        "    sns.boxplot(data=lead_df, x=var)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igPpBYQv3CKC"
      },
      "outputs": [],
      "source": [
        "# To plot numerical variables against target variable to analyse relations\n",
        "plt.figure(figsize=(15, 5))\n",
        "for var in num_var:\n",
        "    plt.subplot(1,3,num_var.index(var)+1)\n",
        "    sns.boxplot(y = var , x = 'Converted', data = lead_df)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8vnqOfL3WRL"
      },
      "source": [
        "#### **Observations:**  \n",
        "- 'TotalVisits' has same median values for both outputs of leads. No conclusion can be drwan from this.\n",
        "- People spending more time on the website are more likely to be converted. This is also aligned with our general knowledge.\n",
        "- 'Page Views Per Visit' also has same median values for both outputs of leads. Hence, inconclusive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZOPINXY33N_"
      },
      "source": [
        "### Visualising Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM5yv-Ac34gD"
      },
      "outputs": [],
      "source": [
        "# Categorical variables\n",
        "cat_var = list(lead_df.columns[lead_df.dtypes == 'object'])\n",
        "cat_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWuoEy614Imr"
      },
      "source": [
        "#### We saw percentage plots for categorical variables while cleaning the data. Here, we'll see these plots with respect to target variable 'Converted'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocuNc7Ok4Jgm"
      },
      "outputs": [],
      "source": [
        "# Functions to plot countplots for categorical variables with target variable\n",
        "\n",
        "# For single plot\n",
        "def plot_cat_var(var):\n",
        "    plt.figure(figsize=(20, 7))\n",
        "    sns.countplot(x = var, hue = \"Converted\", data = lead_df)\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.show()\n",
        "\n",
        "# For multiple plots\n",
        "def plot_cat_vars(lst):\n",
        "    l = int(len(lst)/2)\n",
        "    plt.figure(figsize=(20, l*7))\n",
        "    for var in lst:\n",
        "        plt.subplot(l,2,lst.index(var)+1)\n",
        "        sns.countplot(x = var, hue = \"Converted\", data = lead_df)\n",
        "        plt.xticks(rotation = 90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmLrNF_T7SB5"
      },
      "outputs": [],
      "source": [
        "plot_cat_var(cat_var[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JXLn9gz7pSX"
      },
      "source": [
        "### **Observations for Lead Origin:**  \n",
        "'API' and 'Landing Page Submission' generate the most leads but have less conversion rates of around 30%. Whereas, 'Lead Add Form' generates less leads but conversion rate is great. **We should try to increase conversion rate for 'API' and 'Landing Page Submission', and increase leads generation using 'Lead Add Form'**. 'Lead Import' does not seem very significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGFy9r0V7ZVg"
      },
      "outputs": [],
      "source": [
        "plot_cat_var(cat_var[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnLojeic8bnK"
      },
      "source": [
        "### **Observations for `Lead Source`:**\n",
        "- Spelling error: We've to change 'google' to 'Google'\n",
        "- As it can be seen from the graph, number of leads generated by many of the sources are negligible. There are sufficient numbers till Facebook. We can convert all others in one single category of 'Others'.\n",
        "- 'Direct Traffic' and 'Google' generate maximum number of leads while maximum conversion rate is achieved through 'Reference' and 'Welingak Website'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZlWUScX9WZg"
      },
      "outputs": [],
      "source": [
        "# To correct spelling error\n",
        "lead_df['Lead Source'] = lead_df['Lead Source'].replace(['google'], 'Google')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3BvcPiT9yWM"
      },
      "outputs": [],
      "source": [
        "categories = lead_df['Lead Source'].unique()\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SYSwN3e98Ov"
      },
      "outputs": [],
      "source": [
        "# To reduce categories\n",
        "lead_df['Lead Source'] = lead_df['Lead Source'].replace(categories[8:], 'Others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04P0xBOd-JKx"
      },
      "outputs": [],
      "source": [
        "# To plot new categories\n",
        "plot_cat_var(cat_var[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX9ksh2V-QvQ"
      },
      "outputs": [],
      "source": [
        "plot_cat_vars([cat_var[2],cat_var[3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkJrWN-4-g_o"
      },
      "source": [
        "### **Observations for `Do Not Email` and `Do Not Call`:**  \n",
        "As one can expect, most of the responses are 'No' for both the variables which generated most of the leads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdWnb1Pz-eBG"
      },
      "outputs": [],
      "source": [
        "plot_cat_var(cat_var[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZVMKPJZ-u97"
      },
      "source": [
        "### **Observations for `Last Activity`:**  \n",
        "- Highest number of lead are generated where the last activity is 'Email Opened' while maximum conversion rate is for the activity of 'SMS Sent'. Its conversion rate is significantly high.\n",
        "- Categories after the 'SMS Sent' have almost negligible effect. We can aggregate them all in one single category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-1nS70E-tAo"
      },
      "outputs": [],
      "source": [
        "categories = lead_df['Last Activity'].unique()\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR5-PZZCAZDM"
      },
      "source": [
        "#### We can see that we do not require last five categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi9EKzhdAVwm"
      },
      "outputs": [],
      "source": [
        "# To reduce categories\n",
        "lead_df['Last Activity'] = lead_df['Last Activity'].replace(categories[-5:], 'Others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eo8b761AgxI"
      },
      "outputs": [],
      "source": [
        "# To plot new categories\n",
        "plot_cat_var(cat_var[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO2dGMgBAiul"
      },
      "outputs": [],
      "source": [
        "plot_cat_var(cat_var[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjeQa3hArsg"
      },
      "source": [
        "### **Observations for `Country`:**  \n",
        "Most of the responses are for India. Others are not significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHomDYjYAml4"
      },
      "outputs": [],
      "source": [
        "plot_cat_var(cat_var[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4u28jfdA8Uc"
      },
      "source": [
        "### **Observations for `Specialization`:**  \n",
        "Conversion rates are mostly similar across different specializations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8jm3xqKA7tl"
      },
      "outputs": [],
      "source": [
        "plot_cat_vars([cat_var[7],cat_var[8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0q16LUcBQNv"
      },
      "source": [
        "### **Observations for `What is your current occupation` and `What matters most to you in choosing a course`:**  \n",
        "- The highest conversion rate is for 'Working Professional'. High number of leads are generated for 'Unemployed' but conversion rate is low.\n",
        "- Variable 'What matters most to you in choosing a course' has only one category with significant count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpomVJ1CBMUV"
      },
      "outputs": [],
      "source": [
        "plot_cat_vars(cat_var[9:17])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVhZVpIKCFOL"
      },
      "source": [
        "### **Observations for `Search`, `Magazine`, `Newspaper Article`, `X Education Forums`, `Newspaper`, `Digital Advertisement`, `Through Recommendations`, and `Receive More Updates About Our Courses`:**  \n",
        "As all the above variables have most of the values as no, nothing significant can be inferred from these plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC-qw7o4B0zJ"
      },
      "outputs": [],
      "source": [
        "plot_cat_vars([cat_var[17],cat_var[18]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTzVgY9ACwn3"
      },
      "source": [
        "### **Observations for `Tags` and `Lead Quality`:**  \n",
        "- In Tags, categories after 'Interested in full time MBA' have very few leads generated, so we can combine them into one single category.\n",
        "- Most leads generated and the highest conversion rate are both attributed to the tag 'Will revert after reading the email'.\n",
        "- In Lead quality, as expected, 'Might be' as the highest conversion rate while 'Worst' has the lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJw7_uvxCnAQ"
      },
      "outputs": [],
      "source": [
        "categories = lead_df['Tags'].unique()\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jULNyftFDJUf"
      },
      "source": [
        "#### We can combine that last eight categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihc0ZkBtDF5J"
      },
      "outputs": [],
      "source": [
        "# To reduce categories\n",
        "lead_df['Tags'] = lead_df['Tags'].replace(categories[-8:], 'Others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-60-VygEfdz"
      },
      "outputs": [],
      "source": [
        "# To plot new categories\n",
        "plot_cat_var(cat_var[17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvt3MWy_EiPa"
      },
      "outputs": [],
      "source": [
        "plot_cat_vars(cat_var[19:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRjyWy8bE2GG"
      },
      "source": [
        "### **Observations for `Update me on Supply Chain Content`, `Get updates on DM Content`, `City`, `I agree to pay the amount through cheque`, `A free copy of Mastering The Interview`, and `Last Notable Activity` :\n",
        "\n",
        "- Most of these variables are insignificant in analysis as many of them only have one significant category 'NO'.\n",
        "- In City, most of the leads are generated for 'Mumbai'.\n",
        "In 'A free copy of Mastering The Interview', both categories have similar conversion rates.\n",
        "- In 'Last Notable Activity', we can combine categories after 'SMS Sent' similar to the variable 'Last Activity'. - It has most generated leads for the category 'Modified' while most conversion rate for 'SMS Sent' activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFM-Vj8qEjjS"
      },
      "outputs": [],
      "source": [
        "categories = lead_df['Last Notable Activity'].unique()\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSyaUIwFo3E"
      },
      "source": [
        "#### We can see that we do not require last six categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93X8RTLmFkji"
      },
      "outputs": [],
      "source": [
        "# To reduce categories\n",
        "lead_df['Last Notable Activity'] = lead_df['Last Notable Activity'].replace(categories[-6:], 'Others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbE_VqK8Fxkk"
      },
      "outputs": [],
      "source": [
        "# To plot new categories\n",
        "plot_cat_var(cat_var[24])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqXJgrF-F5rw"
      },
      "source": [
        "#### Based on the data visualization, we can drop the variables which are not significant for analysis and will not any information to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hebNxAQFzUQ"
      },
      "outputs": [],
      "source": [
        "lead_df = lead_df.drop(['Do Not Call','Country','What matters most to you in choosing a course','Search','Magazine','Newspaper Article',\n",
        "                          'X Education Forums','Newspaper','Digital Advertisement','Through Recommendations',\n",
        "                          'Receive More Updates About Our Courses','Update me on Supply Chain Content',\n",
        "                          'Get updates on DM Content','I agree to pay the amount through cheque',\n",
        "                          'A free copy of Mastering The Interview'],axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF44K4xxGDGr"
      },
      "outputs": [],
      "source": [
        "# Final DataFrame\n",
        "lead_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1bIv7bYGJhu"
      },
      "outputs": [],
      "source": [
        "lead_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe-SSbPLGQ0v"
      },
      "outputs": [],
      "source": [
        "lead_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddjMUdw6Gw4j"
      },
      "source": [
        "## Step 4: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvQOZByBGqYH"
      },
      "outputs": [],
      "source": [
        "# To convert binary variable (Yes/No) to 0/1\n",
        "lead_df['Do Not Email'] = lead_df['Do Not Email'].map({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_AI9LgCJBPu"
      },
      "source": [
        "### Dummy Variable Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGtQ-rrJHus"
      },
      "source": [
        "#### For categorical variables with multiple levels, we create dummy features (one-hot encoded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NapxVDmbI6l0"
      },
      "outputs": [],
      "source": [
        "# Categorical variables\n",
        "cat_var = list(lead_df.columns[lead_df.dtypes == 'object'])\n",
        "cat_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYXszjrVJQUm"
      },
      "outputs": [],
      "source": [
        "# Step 1: Convert boolean columns to integers (0 & 1)\n",
        "lead_df = lead_df.astype({col: 'int' for col in lead_df.select_dtypes(include=['bool']).columns})\n",
        "\n",
        "# Step 2: Identify categorical columns correctly\n",
        "cat_var = list(lead_df.select_dtypes(include=['object', 'category']).columns)\n",
        "\n",
        "# Step 3: Apply One-Hot Encoding only if categorical variables exist\n",
        "if cat_var:\n",
        "    dummy = pd.get_dummies(lead_df[cat_var], drop_first=True)\n",
        "    lead_df = pd.concat([lead_df, dummy], axis=1)\n",
        "    lead_df = lead_df.drop(cat_var, axis=1)  # Drop original categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT0-cNHhJg1B"
      },
      "outputs": [],
      "source": [
        "lead_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNxzvOtRJnao"
      },
      "source": [
        "#### Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I7n3b2BJiJI"
      },
      "outputs": [],
      "source": [
        "# Importing required package\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uDPKPg1JwLZ"
      },
      "outputs": [],
      "source": [
        "# To put feature variable to X\n",
        "X = lead_df.drop(['Converted'],axis=1)\n",
        "y = lead_df['Converted']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqFvFI0PKNJ_"
      },
      "outputs": [],
      "source": [
        "# To split the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O40FogVuLvMg"
      },
      "source": [
        "#### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42gQRXOdLTmF"
      },
      "outputs": [],
      "source": [
        "# Importing required package\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkPxw7lmL4Ap"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7cL7OtcL5VS"
      },
      "outputs": [],
      "source": [
        "# Numerical variables\n",
        "num_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ3yoIZbMHxa"
      },
      "outputs": [],
      "source": [
        "#Applying scaler to all numerical columns\n",
        "X_train[num_var] = scaler.fit_transform(X_train[num_var])\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YbaFvNyNFuD"
      },
      "source": [
        "## Step 5: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg81ZtV-NzU9"
      },
      "source": [
        "#### Feature Selection Using RFE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88h33RZdMm_w"
      },
      "outputs": [],
      "source": [
        "# To create an instance of Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7U3Td71N8Mt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "rfe = RFE(logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
        "rfe = rfe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLZak_FwOEsW"
      },
      "outputs": [],
      "source": [
        "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r9wQ56lOM9w"
      },
      "outputs": [],
      "source": [
        "# Features selected\n",
        "col = X_train.columns[rfe.support_]\n",
        "col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTmoCmAgO3Qh"
      },
      "outputs": [],
      "source": [
        "# Features eliminated\n",
        "X_train.columns[~rfe.support_]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-NTamvO9mH"
      },
      "source": [
        "#### Assessing the Model with StatsModels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wN6dGmJPC22"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Function for building the model\n",
        "def build_model(X,y):\n",
        "    X_sm = sm.add_constant(X)    # To add a constant\n",
        "    logm = sm.GLM(y, X_sm, family = sm.families.Binomial()).fit()    # To fit the model\n",
        "    print(logm.summary())    # Summary of the model\n",
        "    return X_sm, logm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Qhg7r-PEtd"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Function to calculate Variance Inflation Factor (VIF)\n",
        "def check_VIF(X_in):\n",
        "    X = X_in.drop('const', axis=1)  # ✅ Fix: Use axis=1\n",
        "    vif = pd.DataFrame()\n",
        "    vif['Features'] = X.columns\n",
        "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    return vif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-GrphZ2PV5m"
      },
      "outputs": [],
      "source": [
        "# Function to get predicted values on train set\n",
        "\n",
        "def get_pred(X,logm):\n",
        "    y_train_pred = logm.predict(X)\n",
        "    y_train_pred = y_train_pred.values.reshape(-1)\n",
        "    # To create a dataframe to store original and predicted values\n",
        "    y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\n",
        "    y_train_pred_final['Lead ID'] = y_train.index\n",
        "    # Using default threshold of 0.5 for now\n",
        "    y_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n",
        "    return y_train_pred_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMYi7Lc-PkVf"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Function to get confusion matrix and accuracy\n",
        "def conf_mat(Converted,predicted):\n",
        "    confusion = metrics.confusion_matrix(Converted, predicted )\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion)\n",
        "    print(\"Training Accuracy: \", metrics.accuracy_score(Converted, predicted))\n",
        "    return confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scO29-LxPyJW"
      },
      "outputs": [],
      "source": [
        "# Function for calculating metric beyond accuracy\n",
        "def other_metrics(confusion):\n",
        "    TP = confusion[1,1]    # True positives\n",
        "    TN = confusion[0,0]    # True negatives\n",
        "    FP = confusion[0,1]    # False positives\n",
        "    FN = confusion[1,0]    # False negatives\n",
        "    print(\"Sensitivity: \", TP / float(TP+FN))\n",
        "    print(\"Specificity: \", TN / float(TN+FP))\n",
        "    print(\"False postive rate: \", FP/ float(TN+FP))\n",
        "    print(\"Positive predictive value: \", TP / float(TP+FP))\n",
        "    print(\"Negative predictive value: \", TN / float(TN+FN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbHSRaWMQQfK"
      },
      "source": [
        "**Model 1**  \n",
        "Running the first model by using the features selected by RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLT0T6hfQsbm"
      },
      "source": [
        "`Tags_invalid number` has a very high p-value > 0.05. Hence, it is insignificant and can be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX5rluKsPIHP"
      },
      "outputs": [],
      "source": [
        "# Convert all boolean columns to int (0,1)\n",
        "X_train = X_train.astype(int)\n",
        "\n",
        "# Ensure all columns are numeric (int or float)\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop any remaining NaN values\n",
        "X_train = X_train.dropna()\n",
        "\n",
        "# Now, call the build_model function\n",
        "X1, logm1 = build_model(X_train[col], y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epMyoX8Qxve"
      },
      "source": [
        "**Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qG4MKKnQVTu"
      },
      "outputs": [],
      "source": [
        "# Ensure all data is numeric (convert bool to int and object columns to numeric)\n",
        "X_train = X_train.astype(int)\n",
        "\n",
        "# Convert all columns to numeric (handle any remaining object data)\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop any remaining NaN values\n",
        "X_train = X_train.dropna()\n",
        "\n",
        "col1 = col.drop(['Tags_invalid number'], errors='ignore')\n",
        "# Now, rebuild the model\n",
        "X2, logm2 = build_model(X_train[col1], y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOns90ndQ8Ho"
      },
      "source": [
        "`Tags_number not provided` has a very high p-value > 0.05. Hence, it is insignificant and can be dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWBDR33hQ_Yq"
      },
      "source": [
        "**Model 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ptviq6PQ1CB"
      },
      "outputs": [],
      "source": [
        "col2 = col1.drop(['Tags_number not provided'], errors='ignore')\n",
        "\n",
        "# To rebuild the model\n",
        "X3, logm3 = build_model(X_train[col2],y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upL-k-GROBy"
      },
      "source": [
        "`Tags_wrong number given` has a very high p-value > 0.05. Hence, it is insignificant and can be dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMnSRy55RQfe"
      },
      "source": [
        "## Model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoYGne29RDaw"
      },
      "outputs": [],
      "source": [
        "col3 = col2.drop(['Tags_wrong number given'],errors='ignore')\n",
        "\n",
        "# To rebuild the model\n",
        "X4, logm4 = build_model(X_train[col3],y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdu9nV2MRiws"
      },
      "source": [
        "### All of the features have p-value close to zero i.e. they all seem significant.\n",
        "\n",
        "We also have to check VIFs (Variance Inflation Factors) of features to see if there's any multicollinearity present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQnLmgTtPIHV"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "def check_VIF(X):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Features\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [round(variance_inflation_factor(X.values, i), 2) for i in range(X.shape[1])]\n",
        "\n",
        "    # Sort values by VIF in descending order\n",
        "    vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Print the output\n",
        "    print(vif_data.to_string(index=False))\n",
        "\n",
        "# Call the function with your dataset X4\n",
        "check_VIF(X4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6NSvxa4SBFr"
      },
      "outputs": [],
      "source": [
        "# To plot correlations\n",
        "plt.figure(figsize = (20,10))\n",
        "sns.heatmap(X4.corr(),annot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX_hswLySDmf"
      },
      "source": [
        "#### From VIF values and heat maps, we can see that there is not much multicollinearity present. All variables have a good value of VIF. These features seem important from the business aspect as well. So we need not drop any more variables and we can proceed with making predictions using this model only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IuU84-cPIHW"
      },
      "outputs": [],
      "source": [
        "# Get the features that were used to train logm4\n",
        "expected_features = logm4.params.index.tolist()\n",
        "\n",
        "# Reorder X4 columns to match the trained model and add missing columns (if any)\n",
        "for col in expected_features:\n",
        "    if col not in X4.columns:\n",
        "        X4[col] = 0  # Add missing columns with default value 0\n",
        "\n",
        "# Ensure the order matches the model's expectations\n",
        "X4 = X4[expected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5C6pBH0SF9W"
      },
      "outputs": [],
      "source": [
        "# To get predicted values on train set\n",
        "y_train_pred_final = get_pred(X4, logm4)\n",
        "y_train_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUg31pPZVvqU"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix and accuracy\n",
        "confusion = conf_mat(y_train_pred_final.Converted,y_train_pred_final.predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7craurFtWEKX"
      },
      "source": [
        "### | Predicted/Actual | Not converted Leads | Converted Leads |\n",
        "    | --- | --- | --- |\n",
        "    | Not converted Leads | 3753 | 152 |\n",
        "    | Converted Leads | 567 | 1879 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBu33H54k6J8"
      },
      "outputs": [],
      "source": [
        "other_metrics(confusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1045s_BlYXkR"
      },
      "source": [
        "This is our **final model**:\n",
        "\n",
        "1.  All p-values are very close to zero.\n",
        "2.  VIFs for all features are very low. There is hardly any multicollinearity present.\n",
        "3.  Training accuracy of 88.67% at a probability threshold of 0.05 is also very good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H6-QDlZaRQ-"
      },
      "source": [
        "## Step 6: Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpscjXQ3ltUt"
      },
      "source": [
        "### Plotting the ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBX170kVl-hi"
      },
      "source": [
        "### An ROC curve demonstrates several things:\n",
        "\n",
        "- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n",
        "- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n",
        "- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw6wzn5BV02Q"
      },
      "outputs": [],
      "source": [
        "# Function to plot ROC\n",
        "def plot_roc(actual,probs):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(actual, probs, drop_intermediate = False)\n",
        "    auc_score = metrics.roc_auc_score(actual, probs)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7fIKpYVmF2i"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kdeq4cCmLRy"
      },
      "outputs": [],
      "source": [
        "# To plot ROC\n",
        "plot_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdZfSm3dmQKA"
      },
      "outputs": [],
      "source": [
        "print(\"Area under curve: \", metrics.roc_auc_score(y_train_pred_final.Converted, y_train_pred_final.Converted_prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGvLW6JmavX"
      },
      "source": [
        "### Area under curve (auc) is approximately 0.94 which is very close to ideal auc of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQCyLXomfzz"
      },
      "source": [
        "#### Finding Optimal Cutoff Point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc81rjfvmlBe"
      },
      "source": [
        "Optimal cutoff probability is the prob where we get balanced sensitivity and specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSaNDIuZmXAs"
      },
      "outputs": [],
      "source": [
        "# To create columns with different probability cutoffs\n",
        "numbers = [float(x)/10 for x in range(10)]\n",
        "for i in numbers:\n",
        "    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
        "y_train_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h18kUGGEqa1l"
      },
      "outputs": [],
      "source": [
        "# To calculate accuracy, sensitivity and specificity for various probability cutoffs\n",
        "cutoff_df = pd.DataFrame(columns = ['prob','accuracy','sensi','speci'])\n",
        "\n",
        "# TP = confusion[1,1]    # True positive\n",
        "# TN = confusion[0,0]    # True negatives\n",
        "# FP = confusion[0,1]    # False positives\n",
        "# FN = confusion[1,0]    # False negatives\n",
        "\n",
        "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "for i in num:\n",
        "    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
        "    total1=sum(sum(cm1))\n",
        "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "\n",
        "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
        "print(cutoff_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhNDkOQUq-0u"
      },
      "outputs": [],
      "source": [
        "# To plot accuracy, sensitivity and specificity for various probabilities\n",
        "sns.set_style('white')\n",
        "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRJ2hG0zrSc0"
      },
      "source": [
        "### From the curve above, **0.2 is the optimum point to take as a cutoff probability**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kQSjOPxrBGc"
      },
      "outputs": [],
      "source": [
        "# Using 0.2 threshold for predictions\n",
        "y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.2 else 0)\n",
        "\n",
        "y_train_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf0O-rAgrIZX"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and Overall Accuracy\n",
        "confusion2 = conf_mat(y_train_pred_final.Converted,y_train_pred_final.final_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDPAmutbrq3L"
      },
      "outputs": [],
      "source": [
        "# To plot confusion matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=confusion2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ugBlQHhrvQ3"
      },
      "outputs": [],
      "source": [
        "# Other metrics\n",
        "other_metrics(confusion2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRylOUFLsPOQ"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwpioNccsIVp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_train_pred_final.Converted, y_train_pred_final.final_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqm1fVmIscO0"
      },
      "source": [
        "## Step 7: Precision and Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmeMm26rsWlm"
      },
      "outputs": [],
      "source": [
        "confusion[1,1]/(confusion[0,1]+confusion[1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G-_ktlNsouQ"
      },
      "source": [
        "### Recall = TP / TP + FN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBfkKlogsmVB"
      },
      "outputs": [],
      "source": [
        "confusion[1,1]/(confusion[1,0]+confusion[1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdgoho7vsuZy"
      },
      "source": [
        "### Using sklearn utilities for the same:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfWcJrcZsryI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGCkUE83syvI"
      },
      "outputs": [],
      "source": [
        "precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zn5jWZgs2OI"
      },
      "outputs": [],
      "source": [
        "recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KijP6YG-s7ch"
      },
      "source": [
        "### Precision and Recall Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLEXTLX8s5db"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Js4rRuQtCMn"
      },
      "outputs": [],
      "source": [
        "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xPX-2pKtLXZ"
      },
      "outputs": [],
      "source": [
        "# To plot precision vs recall for different thresholds\n",
        "plt.plot(thresholds, p[:-1], \"g-\")\n",
        "plt.plot(thresholds, r[:-1], \"r-\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lywJLpo4tPK7"
      },
      "source": [
        "### From the curve above, 0.25 is the optimum point to take as a cutoff probability using Precision-Recall. We can check our accuracy using this cutoff too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-unPr2DstMs5"
      },
      "outputs": [],
      "source": [
        "# Using 0.25 threshold for predictions\n",
        "y_train_pred_final['final_predicted_pr'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.25 else 0)\n",
        "\n",
        "y_train_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVLR_b0otWWZ"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and overall accuracy\n",
        "confusion3 = conf_mat(y_train_pred_final.Converted,y_train_pred_final.final_predicted_pr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPNIi0wHtaPW"
      },
      "outputs": [],
      "source": [
        "# Other metrics\n",
        "other_metrics(confusion3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYWFSHjytd_6"
      },
      "source": [
        "### Accuracy and other metrics yield similar values for both the cutoffs. We'll use the cutoff of 0.25 as derived earlier for predictions on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa4irz3wtgsl"
      },
      "source": [
        "## Step 8: Prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LddInKD9tcdR"
      },
      "outputs": [],
      "source": [
        "# Feature transform on Test set\n",
        "X_test = X_test.astype(int)\n",
        "\n",
        "# Apply feature scaling only on numeric variables\n",
        "X_test[num_var] = scaler.transform(X_test[num_var])\n",
        "\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBN4NLO3GSKu"
      },
      "outputs": [],
      "source": [
        "# To get final features\n",
        "X_test_sm = X_test[col3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fob_kJnGWT_"
      },
      "outputs": [],
      "source": [
        "# Select relevant columns, remove duplicates, and add constant\n",
        "X_test_sm = sm.add_constant(X_test.loc[:, ~X_test.columns.duplicated()][logm4.params.index[1:]], has_constant=\"add\")\n",
        "\n",
        "# Convert everything to float (fixes 'exp' error)\n",
        "X_test_sm = X_test_sm.astype(float)\n",
        "\n",
        "# Check for NaNs and fill with 0 (or other appropriate value)\n",
        "X_test_sm = X_test_sm.fillna(0)\n",
        "\n",
        "# Predict\n",
        "y_test_pred = logm4.predict(X_test_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwcfV03tGefw"
      },
      "outputs": [],
      "source": [
        "# To convert y_pred to a dataframe which is an array\n",
        "y_pred_1 = pd.DataFrame(y_test_pred)\n",
        "\n",
        "y_pred_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ei0v_lnGvyU"
      },
      "outputs": [],
      "source": [
        "# To convert y_test to dataframe\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "\n",
        "# Putting Lead ID to index\n",
        "y_test_df['Lead ID'] = y_test_df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCYO3b1EGyR5"
      },
      "outputs": [],
      "source": [
        "# To remove index for both dataframes to append them side by side\n",
        "y_pred_1.reset_index(drop=True, inplace=True)\n",
        "y_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# To append y_test_df and y_pred_1\n",
        "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n",
        "\n",
        "# To Rename the column\n",
        "y_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})\n",
        "\n",
        "y_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nYjro-bG5QC"
      },
      "outputs": [],
      "source": [
        "# To put the threshold of 0.2 as derived\n",
        "y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.25 else 0)\n",
        "\n",
        "y_pred_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjdNPzOwHSQy"
      },
      "outputs": [],
      "source": [
        "print(\"Area under curve: \", metrics.roc_auc_score(y_pred_final.Converted, y_pred_final.Converted_prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvpdG-ymHW9x"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and overall accuracy\n",
        "confusion_test = conf_mat(y_pred_final.Converted,y_pred_final.final_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhJSTvgUHbuA"
      },
      "outputs": [],
      "source": [
        "# To plot confusion matrix\n",
        "plot_confusion_matrix(conf_mat=confusion_test)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZdWqjuRHhfE"
      },
      "source": [
        " | Predicted/Actual | Not converted Leads | Converted Leads |\n",
        "    | --- | --- | --- |\n",
        "    | Not converted Leads | 1635 | 95 |\n",
        "    | Converted Leads | 158 | 831 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ca_8xt1HdPs"
      },
      "outputs": [],
      "source": [
        "# Other metrics\n",
        "other_metrics(confusion_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm4iI9uoH9GE"
      },
      "source": [
        "### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-fO4SawH7lL"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_pred_final.Converted, y_pred_final.final_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQUiq_iRGL5O"
      },
      "source": [
        "## Step 9: Determining Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ir_8WfKGfAy"
      },
      "source": [
        "### Assigning Lead Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0edu9vD-Gno7"
      },
      "source": [
        "#### Lead Score = 100 * ConversionProbability\n",
        "#### This needs to be calculated for all the leads from the original dataset (train + test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy5YcVkRGYG-"
      },
      "outputs": [],
      "source": [
        "# To select test set\n",
        "leads_test_pred = y_pred_final.copy()\n",
        "leads_test_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upCVjPKRGzkq"
      },
      "outputs": [],
      "source": [
        "# To select train set\n",
        "leads_train_pred = y_train_pred_final.copy()\n",
        "leads_train_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n3YTrvxG2Ne"
      },
      "outputs": [],
      "source": [
        "# To drop unnecessary columns from train set\n",
        "leads_train_pred = leads_train_pred[['Lead ID','Converted','Converted_prob','final_predicted']]\n",
        "leads_train_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdSFZLYfG42w"
      },
      "outputs": [],
      "source": [
        "# To concatenate 2 datasets\n",
        "lead_full_pred = pd.concat([leads_train_pred, leads_test_pred])  # Fix for .append() deprecation\n",
        "lead_full_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9BVANUEG93s"
      },
      "outputs": [],
      "source": [
        "# To inspect the shape of the final dataset\n",
        "print(leads_train_pred.shape)\n",
        "print(leads_test_pred.shape)\n",
        "print(lead_full_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQIpW2JZHDyO"
      },
      "outputs": [],
      "source": [
        "# To ensure uniqueness of Lead IDs\n",
        "len(lead_full_pred['Lead ID'].unique().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxcZZgePHDvK"
      },
      "outputs": [],
      "source": [
        "# To calculate the Lead Score\n",
        "lead_full_pred['Lead_Score'] = lead_full_pred['Converted_prob'].apply(lambda x : round(x*100))\n",
        "lead_full_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgTDN7gFHDr2"
      },
      "outputs": [],
      "source": [
        "# To make the Lead ID column as index\n",
        "lead_full_pred = lead_full_pred.set_index('Lead ID').sort_index(axis = 0, ascending = True)\n",
        "lead_full_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQF-pw-6HDot"
      },
      "outputs": [],
      "source": [
        "# To get Lead Number column from original data\n",
        "leads_original = lead_df_original[['Lead Number']]\n",
        "leads_original.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIChmsqYHDhv"
      },
      "outputs": [],
      "source": [
        "# To concatenate the 2 dataframes based on index\n",
        "leads_with_score = pd.concat([leads_original, lead_full_pred], axis=1)\n",
        "leads_with_score.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK0eiDKXHDS6"
      },
      "outputs": [],
      "source": [
        "# To concatenate the 2 dataframes based on index\n",
        "leads_with_score = pd.concat([leads_original, lead_full_pred], axis=1)\n",
        "leads_with_score.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSmjALEhJDY8"
      },
      "source": [
        "#### We have a new data frame consisting of Lead Number and Lead Score. Lead Number will help in easy referencing with the original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPa8BiWSJIM6"
      },
      "source": [
        "#### Determining Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdBxLtcJCD4"
      },
      "outputs": [],
      "source": [
        "# To display features with corrsponding coefficients in final model\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "new_params = logm4.params[1:]\n",
        "new_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipEdaB2PJeHD"
      },
      "outputs": [],
      "source": [
        "# Relative feature importance\n",
        "feature_importance = new_params\n",
        "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "feature_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyr_puOvJeER"
      },
      "outputs": [],
      "source": [
        "# To sort features based on importance\n",
        "sorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\n",
        "sorted_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvUMDXFyJeBE"
      },
      "outputs": [],
      "source": [
        "# To plot features with their relative importance\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "pos = np.arange(sorted_idx.shape[0])\n",
        "ax.barh(pos, feature_importance[sorted_idx])\n",
        "ax.set_yticks(pos)\n",
        "ax.set_yticklabels(np.array(X_train[col3].columns)[sorted_idx], fontsize=12)\n",
        "ax.set_xlabel('Relative Feature Importance', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVt4qlA_ITZq"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObTNSw6pJsu9"
      },
      "source": [
        "### After trying out saveral models, our final model has following characteristics:  \n",
        "1. All p-values are very close to zero.\n",
        "2. VIFs for all features are very low. There is hardly any multicollinearity present.\n",
        "3. The overall testing accuracy of 90.67% at a probability threshold of 0.05 is also very good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxNteCqzJsnS"
      },
      "source": [
        "| Dataset | Accuracy | Sensitivity | Specificity | False Positive Rate | Positive Predictive Rate | Negative Predictive Value|  AUC |\n",
        "| ------- | -------- | ----------- | ----------- | ------------------ | ----------------------- | ------------------------ |    --- |\n",
        "| Train   | 0.9104   |  0.8597     | 0.9421      | 0.0578             | 0.9829                  | 0.9147                   | 0.9393 |\n",
        "| Test    | 0.9067   |  0.8432     | 0.9429      | 0.0570             | 0.8938                  | 0.9134                   | 0.9454 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0nZBtNCJsgB"
      },
      "source": [
        "The **optimal threshold** for the model is **0.25** which is calculated based on tradeoff between sensitivity, specificity and accuracy. According to business needs, this threshold can be changed to increase or decrease a specific metric.  \n",
        "\n",
        "\n",
        "High sensitivity ensures that most of the leads who are likely to convert are correctly predicted, while high specificity ensures that most of the leads who are not likely to convert are correctly predicted.  \n",
        "\n",
        "\n",
        "**Twelve features** were selected as the most significant in predicting the conversion:  \n",
        "- Features having positive impact on conversion probability in decreasing order of impact:  \n",
        "\n",
        "\n",
        "|**Features with Positive Coefficient Values**|\n",
        "|-|\n",
        "|Tags_Lost to EINS|\n",
        "|Tags_Closed by Horizzon|\n",
        "|Tags_Will revert after reading the email|\n",
        "|Tags_Busy|\n",
        "|Lead Source_Welingak Website|\n",
        "|Last Notable Activity_SMS Sent|\n",
        "|Lead Origin_Lead Add Form|\n",
        "\n",
        "\n",
        "- Features having negative impact on conversion probability in decreasing order of impact:  \n",
        "\n",
        "|**Features with Negative Coefficient Values**|\n",
        "|-|\n",
        "|Lead Quality_Worst|\n",
        "|Lead Quality_Not Sure|\n",
        "|Tags_switched off|\n",
        "|Tags_Ringing|\n",
        "|Do Not Email|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMKL6iP3PIL_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K_PCJny-H2Xp",
        "aTFNgSG28uQN",
        "S85Zg-X-mFtF",
        "ddjMUdw6Gw4j",
        "1YbaFvNyNFuD",
        "iqm1fVmIscO0",
        "qa4irz3wtgsl"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}